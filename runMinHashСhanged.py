# = = = = = = = runMinHashExample ======
# В этом примере кода демонстрируется сравнение документов с помощью MinHash подхода.

# Во-первых, каждый документ представлен набором шинглов. 
# ***Шингл – это определенные фрагменты текста, по которым производится			  ***
# ***проверка оригинальности документа, сайтами проверяющими текст на антиплагиат.***
# тут можно прочитать про шинглы https://анти-антиплагиат.рф/chto-takoe-shingl

# Затем документы можно сравнить, используя сходство по Jaccard 
# Это точный похдох, но очень долгий для болльшо количества документов

# Для сравнения, мы также будем использовать алгоритм MinHash для вычисления сигнатур представления доков
# Эти MinHash сигнатуры могут быстро сравниваться 
# Мы сравним все возможные пары документов, и находим пары с высоким сходством.


# Программа выполняет следующие шаги:
# 1. Преобразуйте каждый тестовый файл в набор шинглов.
#	- Шинглы получаются путем объединения трех последовательных слов вместе.
#	- Шинглы преобразуются в их ID(индефикаторы), используя хэш CRC32.
# ***тут можешь почитать про данное хеширование https://ru.wikipedia.org/wiki/Циклический_избыточный_код ***
# ***но это тебе не обязательно, просто понимай, что такое хеш-функция https://habr.com/ru/post/93226/   ***

# 2. Вычисляем все сходства Jaccard напрямую.
#	- Это нормально для небольших размеров набора данных. 

# 3. Рассчитываем сигнатуры MinHash для каждого документа.
#	- Алгоритм MinHash реализован с использованием случайной хэша функции
#	  Это сделано для некой оптимизации, чтобы не делать все перестановки
#	  раздел 3.3.5 http://infolab.stanford.edu/~ullman/mmds/ch3.pdf

# 4. Сравниваем все сигнатуры MinHash друг с другом.
#	- Сравнение сигнатур MinHash путем подсчета количества компонентов с одинаковыми сигнатурами. 
#	- Отображение пар документов/сигнатуры с большим чем порог сходством.

from __future__ import division
import os
import re
import random # Для генерации случайных значений (случайные хеш-функции)
import time # для определения времени алгоритмов
import binascii #Для перекодировки строк
from bisect import bisect_right #Для вставки элементов в списки
from heapq import heappop, heappush # *** Можешь почитать что такое кучи https://ru.wikipedia.org/wiki/Куча_(структура_данных) ***

# Это число компонентов в результирующих сигнатурах MinHash.
# Соответственно, это также число случайных хэш-функций, которые
# нам понадобится для того, чтобы рассчитать Минхаш.
numHashes = 10;

# Тут мы определяем файлы с скоторыми работаем и их названия
numDocs = 1000
dataFile = "./data/articles_" + str(numDocs) + ".train"
truthFile = "./data/articles_" + str(numDocs) + ".truth"

# =============================================================================
#			Разбирайте Наземные Таблицы Правды
# =============================================================================
# Строим словарь плагиаторов
plagiaries = {}

# Откройте файл правды.
f = open(truthFile, "rU")

# Для каждой строки файла "truthFile"...
for line in f:
  
  # Удаляем символ новой строки(перехода на новуб строку "Enter"), если он присутствует.
  if line[-1] == '\n':
      line = line[0:-1]
      
  docs = line.split(" ") # тут мы разделяем строки по пробелам(на слова)

  # Сопоставьте эти два документа друг с другом.
  plagiaries[docs[0]] = docs[1]
  plagiaries[docs[1]] = docs[0]

# =============================================================================
# Преобразование документов в наборы шинглов ---- 1.1 и 1.2
# =============================================================================

print "Shingling articles..."

# Текущее значение ID шингла 
# При добавлении нового шингла - увеличиваем это значение 
curShingleID = 0

# Создаем словарь статей, сопоставив идентификатор статьи (например,
# "t8470") к списку идентификаторов шинглов
docsAsShingleSets = {};
  
# Открываем файл данных.
f = open(dataFile, "rU")

docNames = [] #Тут храним названия доков

t0 = time.time() #фиксируем первоначальное время

totalShingles = 0 #Всего шинглов по всем документам

for i in range(0, numDocs):
  
  # Читаем все слова (они все на одной строке) и разделяем их
  words = f.readline().split(" ") 
  
  # Извлеките идентификатор статьи, который является первым словом в строке.
  docID = words[0] #Посмотри как выглядят входные данные - сама поймешь
  
  # Добавление в список идентификатор документа.  
  docNames.append(docID)
    
  del words[0]  
  
  # 'shinglesInDoc' будет содержать все уникальные(без дубликатов) ID шинглов, присутствующие в текущем документе. 

  shinglesInDoc = set() #  ***если не знакома с Set - советую ознакомится - крайне полезная вещь https://pythonworld.ru/tipy-dannyx-v-python/mnozhestva-set-i-frozenset.html 
  
  # Для каждого слова в документе...
  for index in range(0, len(words) - 2):
    # Строим шинглы, объединяя по 3 слова
    shingle = words[index] + " " + words[index + 1] + " " + words[index + 2]

    # Тут происходит хеширование  -> в итоге получаем струшную 32-разрядную штуку на каждый шингл - это есть хеш-сумма
    crc = binascii.crc32(shingle) & 0xffffffff #0xffffffff нужен нам для приведения к 32-разрядному виду
    
    # Добавляем хэш-значение в список сигнатур для текущего документа.
	# опять все без дубликатов
    shinglesInDoc.add(crc)
  
  # Сохраните полный список сигнатур для этого документа в словаре.
  docsAsShingleSets[docID] = shinglesInDoc
  
  # Подсчитайте общее количество сигнутур по всем документам.
  totalShingles = totalShingles + (len(words) - 2)

# Закрываем файл.  
f.close()  

# Печатаем вреимя затраченное на создание словаря сигнатур
print '\nShingling ' + str(numDocs) + ' docs took %.2f sec.' % (time.time() - t0)
 
print '\nAverage shingles per doc: %.2f' % (totalShingles / numDocs)

# =============================================================================
# Определяем Треугольную Матрицу 
# =============================================================================


# Определите виртуальной треугольной матрицы для хранения значений подобия. 
# Используем треугольную потому, что пересечений у нас хватает на заполнение треугольной
# а остальная память не забивается и мы не имеем к ней доступ

# *** Пример матрицы пересечений *** где Pij функицй пересечений iого и jого элемента
# 0 1    2    3    4    5    6  ...
# 1 P11
# 2 P12 P22 
# 3 P13 P23  P33
# 4 P14 P24  P34  P44
# 5 P15 P25  P35  P45  P55
# 6 P16 P26  P36  P46  P56  P66
# ...


# Вычислите количество элементов, необходимых в нашей матрице треугольников
numElems = int(numDocs * (numDocs - 1) / 2)

# Инициализируйте два пустых списка для хранения значений подобия.
# 'JSim' будет для фактических значений сходства Jaccard.
# "estJSim" будет для оцененных сходств Jaccard найденных по MinHash
JSim = [0 for x in range(numElems)]
estJSim = [0 for x in range(numElems)]

# Jghtltkztv функцию для получения 2D-координаты в матрице по индексу. -> такой перевод матрицы в вектор
def getTriangleIndex(i, j):
  # Если i = = j, то это ошибка.
  if i == j:
    sys.stderr.write("Can't access triangle matrix with i == j")
    sys.exit(1)
  # Если j < i меняем их местами
  if j < i:
    temp = i
    i = j
    j = temp
  
  # Вычислите индекс внутри треугольного массива.
  # Эта причудливая схема индексирования взята из pg. 211 из:
  # http://infolab.stanford.edu/~ullman/mmds/ch6.pdf
  # Но я адаптировал его для индекса, основанного на 0.
  # Примечание: деление на два не должно усекать, это требует типа float (с плавающей точкой)
  k = int(i * (numDocs - (i + 1) / 2.0) + j - i) - 1
  
  return k


# =============================================================================
#  Вычисляем Jaccar коэффициенты ---- 2
# =============================================================================
# В этом раздел, мы сразу высчитаем сходства 
# Это включено здесь, чтобы показать, насколько это медленнее, чем  MinHash подход.


# Вычисление Jaccard сходства становится очень долгим для большого количества документов.
if numDocs <= 2500:
#делаем если документов меньше 2500 - иначе вычисления уйдут за 20 мин:
    print "\nCalculating Jaccard Similarities..."

    # фиксируем текущее время
    t0 = time.time()

    # Для каждой пары документов...
    for i in range(0, numDocs):
      
      # Печатаем прогресс каждые 100 документов - своеобразный лог(ход) выполнениея 
      if (i % 100) == 0:
        print "  (" + str(i) + " / " + str(numDocs) + ")"

      # Извлекаем набор шинглов для документа i.
      s1 = docsAsShingleSets[docNames[i]]
      
      for j in range(i + 1, numDocs):
        # Извлеките набор шинглов для документа j.
        s2 = docsAsShingleSets[docNames[j]]
        
        # Вычисляем фактическое сходство Jaccard.
        JSim[getTriangleIndex(i, j)] = (len(s1.intersection(s2)) / len(s1.union(s2)))    # заполняем матрицу вычисляя сходства перессечений элементов наборов шинглов

    # Вычислять прошедшее время (в секундах)
    elapsed = (time.time() - t0)
        
    print "\nCalculating all Jaccard Similarities took %.2fsec" % elapsed

# Удалить приложение Jaccard сходства, так как это довольно большая матрица.  
del JSim
        
# =============================================================================
# Создание Подписей MinHash  ---- 3
# =============================================================================

# Запонимаем время начала.
t0 = time.time()

print '\nGenerating random hash functions...'

# Записываем максимальный ID шингла -> он получается таким из-за 32-разрядного преобразования при вычислении хеша шинглов
maxShingleID = 2**32-1

# Нам нужно следующее простое число над 'maxshinleid'. Записываем его в nextPrime
nextPrime = 4294967311 #Для вычисления хеш-суммы


# Наш случайный хэш функция примет вид::
# h (x) = (a * x + b) % c
# Где ' x ' находится входное значение, 'a' и ' b '- случайные коэффициенты, а' c ' - это
# простое число - nextPrime


# Создаем список из ' K ' случайных коэффициентов для случайных хэш-функций,
# пока обеспечивающ что одно и то же значение не появляется несколько раз в поле
# список.
def pickRandomCoeffs(k):
  # Создать список из ' K ' случайных величин.
  randList = []
  
  while k > 0:
    # Получаем случайный ID шингла
    randIndex = random.randint(0, maxShingleID) 
  
    # убеждаемся, что каждое случайное число уникально.
    while randIndex in randList: # пока случайное значение есть уже в списке
      randIndex = random.randint(0, maxShingleID) 
    
    # Добавляем случайное число в списке.
    randList.append(randIndex)
    k = k - 1
    
  return randList

# Для каждого из них 'numHashes' хэш-функции, генерировать другой коэффициент ' a ' и 'b'. 
# Значение мы определили в начале программы - 10
coeffA = pickRandomCoeffs(numHashes)
coeffB = pickRandomCoeffs(numHashes)

print '\nGenerating MinHash signatures for all documents...'

# Список из документы, представленные в виде векторов сигнатур
signatures = []


# *** Суть MinHash функции ***
# Вместо того чтобы генерировать все возможные случайные перестановки шинглов,
# мы просто будем хэшировать идентификаторы шинглов, которые находятся * фактически в документе*,
# тогда возьмем самое низкое результирующее значение хэш-кода. Это соответствует индексу
# из первых шингла, с которым вы бы столкнулись в случайном порядке.

# Для каждого документа...
for docID in docNames:
  
  # получаем список ID шинглов для этого документа
  shingleIDSet = docsAsShingleSets[docID]
  
  # Полученный результат minhash для этого документа.
  signature = []
  
  # Для каждого случайных хэш-функции...
  for i in range(0, numHashes):
    
    # Для каждого шингла в документе, вычисляем его хэш-код используя хэш функцию "i".
    
    # Для того, чтобы найти минимальный HashCode изначально заполняем его числом больше возможного для сравнения
    minHashCode = nextPrime + 1
    
    # Проходмся по каждому шинглу
    for shingleID in shingleIDSet:
      # считаем хеш-сумму
      hashCode = (coeffA[i] * shingleID + coeffB[i]) % nextPrime # сама хеш-функция
      
      # забираем минимальное значение
      if hashCode < minHashCode:
        minHashCode = hashCode

    # передаем минимальное значение хеш-функций для данного i
    signature.append(minHashCode)
  
  # Храним ренения финкции MinHash
  signatures.append(signature)

# Считаем Время выполения
elapsed = (time.time() - t0)
        
print "\nGenerating MinHash signatures took %.2fsec" % elapsed  

# =============================================================================
#                     Сравниваем все сигнатуры
# =============================================================================  

print '\nComparing all signatures...'  
  
# Создаем матрицу N x N заполненную нулями

# Фиксируем время текущего шага
t0 = time.time()

# Идем по всем документам (спискам хеш результатов)
for i in range(0, numDocs):
  # Получаем MinHash сигнатуру i-ого документа
  signature1 = signatures[i]
    
  # Идем по другим документам
  for j in range(i + 1, numDocs):
    
    # Получаем MinHash сигнатуру другого документа
    signature2 = signatures[j]
    
    count = 0
    # count выступает в роли счетчика совпадений хеш-кодов -> при равных значения добавляет 1
    for k in range(0, numHashes):
      count = count + (signature1[k] == signature2[k])
    
    # Заполняем матрицу пересечений для Jaccard коэффициентов найденных методом MinHash
    estJSim[getTriangleIndex(i, j)] = (count / numHashes)

# Считаем затраченное время 
elapsed = (time.time() - t0)
        
print "\nComparing MinHash signatures took %.2fsec" % elapsed  
    
    
# =============================================================================
#                   Отображаем пары похожих документов
# =============================================================================  

# Count the true positives and false positives.
# Число пар истенных и ложноположительных результатов
tp = 0
fp = 0
  
threshold = 0.5  
print "\nList of Document Pairs with J(d1,d2) more than", threshold
print "Values shown are the estimated Jaccard similarity and the actual"
print "Jaccard similarity.\n"
print "                   Est. J   Act. J"


#тут уже идет тупо вывод результатов, и подсчет тех, что превосходят порог (есть плагиат) и те, которые 
# находятся в рамках нормы


# Идем по всем парам
for i in range(0, numDocs):  
  for j in range(i + 1, numDocs):
    # Retrieve the estimated similarity value for this pair.
    estJ = estJSim[getTriangleIndex(i, j)]
    
    # If the similarity is above the threshold...
    if estJ > threshold: # сравнение с порогом
    
      # Calculate the actual Jaccard similarity for validation.
      s1 = docsAsShingleSets[docNames[i]]
      s2 = docsAsShingleSets[docNames[j]]
      J = (len(s1.intersection(s2)) / len(s1.union(s2)))
      
      # Print out the match and similarity values with pretty spacing.
      print "  %5s --> %5s   %.2f     %.2f" % (docNames[i], docNames[j], estJ, J)
      
      # Check whether this is a true positive or false positive.
      # We don't need to worry about counting the same true positive twice
      # because we implemented the for-loops to only compare each pair once.
      if plagiaries[docNames[i]] == docNames[j]:
        tp = tp + 1
      else:
        fp = fp + 1

# Display true positive and false positive counts.
print
print "True positives:  " + str(tp) + " / " + str(int(len(plagiaries.keys()) / 2))
print "False positives: " + str(fp)
